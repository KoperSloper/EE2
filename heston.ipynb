{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c37904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def simulate_heston_one_day(mu=0.05,\n",
    "                            kappa=5.0,\n",
    "                            theta=0.04,\n",
    "                            xi=0.5,\n",
    "                            rho=-0.5,\n",
    "                            v0=0.04,\n",
    "                            dt=None):\n",
    "    \"\"\"\n",
    "    Simulate a single day's log-price X, variance v, and volatility sigma\n",
    "    under Heston (1993) at 1-second frequency.\n",
    "    \"\"\"\n",
    "    seconds_per_day = int(6.5 * 3600)             # 23,400 seconds\n",
    "    if dt is None:\n",
    "        dt = (1/252) / seconds_per_day            # time‐step in years\n",
    "\n",
    "    N = seconds_per_day + 1\n",
    "    X = np.empty(N)\n",
    "    v = np.empty(N)\n",
    "    sigma = np.empty(N)\n",
    "\n",
    "    X[0] = np.log(100.0)\n",
    "    v[0] = v0\n",
    "    sigma[0] = np.sqrt(v0)\n",
    "\n",
    "    cov = np.array([[1, rho], [rho, 1]])\n",
    "    L = np.linalg.cholesky(cov)\n",
    "\n",
    "    for t in range(1, N):\n",
    "        z = np.random.randn(2)\n",
    "        dW1, dW2 = (L @ z) * np.sqrt(dt)\n",
    "        v_prev = v[t-1]\n",
    "        dv = kappa * (theta - v_prev) * dt + xi * np.sqrt(max(v_prev,0)) * dW2\n",
    "        v[t] = max(v_prev + dv, 0)\n",
    "        sigma[t] = np.sqrt(v[t])\n",
    "        X[t] = X[t-1] + (mu - 0.5 * v_prev)*dt + sigma[t-1]*dW1\n",
    "\n",
    "    return X, v, sigma\n",
    "\n",
    "def add_micro_noise(X, noise_sd=0.0005):\n",
    "    return X + np.random.normal(0, noise_sd, size=X.shape)\n",
    "\n",
    "def generate_time_varying_noise(variances, rho_eps):\n",
    "    \"\"\"\n",
    "    Generate a zero-mean Gaussian noise series {eps[t]} with\n",
    "    Var(eps[t]) = variances[t] and Corr(eps[t], eps[t-1]) = rho_eps.\n",
    "    We use an AR(1)-style construction on standardized normals:\n",
    "      u[t] = rho_eps * u[t-1] + sqrt(1 - rho_eps^2) * z[t], z[t] ~ N(0,1)\n",
    "      eps[t] = sqrt(variances[t]) * u[t].\n",
    "    \"\"\"\n",
    "    N = variances.shape[0]\n",
    "    u = np.zeros(N)\n",
    "    eps = np.zeros(N)\n",
    "    # Initialize u[0] ~ N(0,1)\n",
    "    u[0] = np.random.randn()\n",
    "    eps[0] = np.sqrt(variances[0]) * u[0]\n",
    "    # Iterate\n",
    "    for t in range(1, N):\n",
    "        z = np.random.randn()\n",
    "        u[t] = rho_eps * u[t-1] + np.sqrt(1 - rho_eps**2) * z\n",
    "        eps[t] = np.sqrt(variances[t]) * u[t]\n",
    "    return eps\n",
    "\n",
    "def subsampled_rv(Y, K):\n",
    "    \"\"\"\n",
    "    Return:\n",
    "      - the average of K subsampled realized variances\n",
    "      - the list of sample‐sizes (n_k) for each of the K grids\n",
    "    \"\"\"\n",
    "    subs = [np.sum(np.diff(Y[k::K])**2) for k in range(K)]\n",
    "    counts = [len(Y[k::K]) - 1 for k in range(K)]\n",
    "    return np.mean(subs), counts\n",
    "\n",
    "def naively_sampled_rv(Y):\n",
    "    return np.sum(np.diff(Y)**2)\n",
    "\n",
    "def zma_rv(Y, K):\n",
    "    \"\"\"\n",
    "    Compute the original ZMA:\n",
    "      ZMA = max{ (n * avg_sub - bar_n * full) / (n - bar_n), 0 }.\n",
    "    Returns the ZMA estimate.\n",
    "    \"\"\"\n",
    "    n = len(Y) - 1\n",
    "    full = naively_sampled_rv(Y)\n",
    "    subsrv, counts = subsampled_rv(Y, K)\n",
    "    avg_sub = subsrv\n",
    "    bar_n = np.mean(counts)\n",
    "    z = (n * avg_sub - bar_n * full) / (n - bar_n)\n",
    "    return max(z, 0.0)\n",
    "\n",
    "def monte_carlo_heston(M=1000,\n",
    "                                 mu=0.05, kappa=5.0, theta=0.04,\n",
    "                                 xi=0.5, rho=-0.5, noise_sd=0.0005):\n",
    "    seconds = int(6.5*3600)\n",
    "    dt = (1/252)/seconds\n",
    "    T = 1/252\n",
    "    var_eps = noise_sd**2\n",
    "\n",
    "    results = {\n",
    "        'Naive':    np.empty(M),\n",
    "        'Subsample':np.empty(M),\n",
    "        'ZMA':      np.empty(M),\n",
    "        'FiveMin':  np.empty(M),\n",
    "        'TrueIV':   np.empty(M),\n",
    "        'K_sub':    np.empty(M),\n",
    "        'K_zma':    np.empty(M)\n",
    "    }\n",
    "\n",
    "    for i in range(M):\n",
    "        X, v, sigma = simulate_heston_one_day(mu=mu, kappa=kappa,\n",
    "                                              theta=theta, xi=xi,\n",
    "                                              rho=rho, v0=theta, dt=dt)\n",
    "        Y = add_micro_noise(X, noise_sd)\n",
    "\n",
    "        iv = np.sum(v[:-1]) * dt\n",
    "        iq = np.sum(v[:-1]**2) * dt\n",
    "\n",
    "        bar_n_star = ((T/(6*var_eps**2)) * iq)**(1/3)\n",
    "        K_sub = max(1, int(round(seconds / bar_n_star)))\n",
    "\n",
    "        eta2 = (4/3) * iq\n",
    "        c_star = ((16 * var_eps**2) / (T * eta2))**(1/3)\n",
    "        K_zma = max(1, int(round(c_star * seconds**(2/3))))\n",
    "\n",
    "        results['Naive'][i]     = naively_sampled_rv(Y)\n",
    "        results['Subsample'][i] = subsampled_rv(Y, K_sub)[0]\n",
    "        results['ZMA'][i]       = zma_rv(Y, K_zma)\n",
    "\n",
    "        idx_5min = np.arange(0, seconds + 1, 300) \n",
    "        Y_5min = Y[idx_5min]\n",
    "        results['FiveMin'][i]   = np.sum(np.diff(Y_5min)**2)\n",
    "\n",
    "        results['TrueIV'][i]    = iv\n",
    "        results['K_sub'][i]     = K_sub\n",
    "        results['K_zma'][i]     = K_zma\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "df = monte_carlo_heston(M=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7472767f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Method          Bias      Variance      RMSE\n",
      "0      Naive  1.169730e-02  1.736962e-08  0.011698\n",
      "1  Subsample  1.556131e-05  8.145025e-10  0.000033\n",
      "2    FiveMin  3.727108e-05  1.070758e-09  0.000050\n",
      "3        ZMA -3.540409e-07  7.919795e-11  0.000009\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(df):\n",
    "    methods = ['Naive', 'Subsample', 'FiveMin', 'ZMA']\n",
    "    rows = []\n",
    "    for m in methods:\n",
    "        diff = df[m] - df['TrueIV']\n",
    "        bias     = diff.mean()\n",
    "        variance = diff.var(ddof=0)      \n",
    "        rmse     = np.sqrt((diff**2).mean())\n",
    "        rows.append({\n",
    "            'Method':   m,\n",
    "            'Bias':     bias,\n",
    "            'Variance': variance,\n",
    "            'RMSE':     rmse\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "metrics_df = compute_metrics(df)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0910a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heteroskedastic, Uncorrelated Noise:\n",
      "      Naive  Subsample   FiveMin       ZMA    TrueIV\n",
      "0  0.011859   0.000302  0.000252  0.000194  0.000184\n",
      "1  0.011861   0.000173  0.000167  0.000141  0.000149\n",
      "2  0.011818   0.000209  0.000187  0.000138  0.000144\n",
      "3  0.011885   0.000226  0.000258  0.000175  0.000174\n",
      "4  0.012172   0.000193  0.000222  0.000200  0.000197 \n",
      "\n",
      "Homoskedastic, Serially Correlated Noise:\n",
      "      Naive  Subsample   FiveMin  ZMA    TrueIV\n",
      "0  0.018010   0.000181  0.000210  0.0  0.000163\n",
      "1  0.017372   0.000159  0.000191  0.0  0.000150\n",
      "2  0.017859   0.000171  0.000168  0.0  0.000155\n",
      "3  0.017275   0.000115  0.000167  0.0  0.000131\n",
      "4  0.017442   0.000212  0.000205  0.0  0.000163 \n",
      "\n",
      "Heteroskedastic & Serially Correlated Noise:\n",
      "      Naive  Subsample   FiveMin  ZMA    TrueIV\n",
      "0  0.017780   0.000214  0.000183  0.0  0.000168\n",
      "1  0.017805   0.000154  0.000165  0.0  0.000165\n",
      "2  0.018396   0.000145  0.000158  0.0  0.000160\n",
      "3  0.017933   0.000224  0.000293  0.0  0.000195\n",
      "4  0.017879   0.000180  0.000239  0.0  0.000165\n"
     ]
    }
   ],
   "source": [
    "def monte_carlo_heston_noise_variations(\n",
    "    M=500,\n",
    "    mu=0.05, kappa=5.0, theta=0.04,\n",
    "    xi=0.5, rho=-0.5,\n",
    "    rho_eps=0.0,\n",
    "    base_noise_var=0.0005**2,\n",
    "    noise_var_amp=0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Monte Carlo over M days for different noise scenarios:\n",
    "    - If noise_var_amp > 0, noise variance oscillates sinusoidally.\n",
    "    - If rho_eps != 0, noise is AR(1) with serial correlation.\n",
    "    Otherwise noise is i.i.d. homoskedastic.\n",
    "    \"\"\"\n",
    "    seconds = int(6.5 * 3600)\n",
    "    dt = (1/252) / seconds\n",
    "    T = 1 / 252\n",
    "\n",
    "    results = {\n",
    "        'Naive':     np.empty(M),\n",
    "        'Subsample': np.empty(M),\n",
    "        'FiveMin':   np.empty(M),\n",
    "        'ZMA':       np.empty(M),\n",
    "        'TrueIV':    np.empty(M),\n",
    "    }\n",
    "\n",
    "    for i in range(M):\n",
    "        X, v, sigma = simulate_heston_one_day(mu=mu, kappa=kappa, theta=theta,\n",
    "                                              xi=xi, rho=rho, v0=theta, dt=dt)\n",
    "\n",
    "        N = X.shape[0]\n",
    "        t_idx = np.arange(N)\n",
    "        variances = base_noise_var * (1 + noise_var_amp * np.sin(2 * np.pi * t_idx / N))\n",
    "\n",
    "        eps = generate_time_varying_noise(variances, rho_eps)\n",
    "\n",
    "        Y = X + eps\n",
    "\n",
    "        iv = np.sum(v[:-1] * dt)\n",
    "\n",
    "        avg_var_eps = np.mean(variances)\n",
    "        iq = np.sum((v[:-1]**2) * dt)\n",
    "        bar_n_star = ((T / (6 * avg_var_eps**2)) * iq)**(1/3)\n",
    "        K_sub = max(1, int(round(seconds / bar_n_star)))\n",
    "\n",
    "        eta2 = (4/3) * iq\n",
    "        c_star = ((16 * avg_var_eps**2) / (T * eta2))**(1/3)\n",
    "        K_zma = max(1, int(round(c_star * (seconds**(2/3)))))\n",
    "\n",
    "        results['Naive'][i]     = naively_sampled_rv(Y)\n",
    "        results['Subsample'][i] = subsampled_rv(Y, K_sub)[0]\n",
    "        idx_5min = np.arange(0, seconds+1, 300)\n",
    "        results['FiveMin'][i]   = np.sum(np.diff(Y[idx_5min])**2)\n",
    "        results['ZMA'][i]       = zma_rv(Y, K_zma)\n",
    "        results['TrueIV'][i]    = iv\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "df_hetero = monte_carlo_heston_noise_variations(\n",
    "    M=1000, rho_eps=0.0, noise_var_amp=0.5\n",
    ")\n",
    "\n",
    "df_serial = monte_carlo_heston_noise_variations(\n",
    "    M=1000, rho_eps=-0.5, noise_var_amp=0.0\n",
    ")\n",
    "\n",
    "df_both = monte_carlo_heston_noise_variations(\n",
    "    M=1000, rho_eps=-0.5, noise_var_amp=0.5\n",
    ")\n",
    "\n",
    "print(\"Heteroskedastic, Uncorrelated Noise:\")\n",
    "print(df_hetero.head(), \"\\n\")\n",
    "print(\"Homoskedastic, Serially Correlated Noise:\")\n",
    "print(df_serial.head(), \"\\n\")\n",
    "print(\"Heteroskedastic & Serially Correlated Noise:\")\n",
    "print(df_both.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fef792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Method          Bias      Variance      RMSE             Scenario\n",
      "0       Naive  1.169529e-02  2.021994e-08  0.011696      Heteroskedastic\n",
      "1   Subsample  1.681428e-05  8.439256e-10  0.000034      Heteroskedastic\n",
      "2     FiveMin  4.119314e-05  1.021049e-09  0.000052      Heteroskedastic\n",
      "3         ZMA  1.380602e-07  8.048659e-11  0.000009      Heteroskedastic\n",
      "4       Naive  1.753229e-02  6.701163e-08  0.017534  Serially Correlated\n",
      "5   Subsample  1.486594e-05  7.814220e-10  0.000032  Serially Correlated\n",
      "6     FiveMin  3.938688e-05  1.111776e-09  0.000052  Serially Correlated\n",
      "7         ZMA -1.585861e-04  1.982600e-10  0.000159  Serially Correlated\n",
      "8       Naive  1.755344e-02  7.297553e-08  0.017556                 Both\n",
      "9   Subsample  1.666656e-05  7.719333e-10  0.000032                 Both\n",
      "10    FiveMin  3.871690e-05  1.082776e-09  0.000051                 Both\n",
      "11        ZMA -1.595753e-04  2.103814e-10  0.000160                 Both\n"
     ]
    }
   ],
   "source": [
    "metrics_hetero = compute_metrics(df_hetero)\n",
    "metrics_serial = compute_metrics(df_serial)\n",
    "metrics_both   = compute_metrics(df_both)\n",
    "\n",
    "metrics_hetero['Scenario'] = 'Heteroskedastic'\n",
    "metrics_serial['Scenario'] = 'Serially Correlated'\n",
    "metrics_both['Scenario']   = 'Both'\n",
    "\n",
    "metrics_all = pd.concat([metrics_hetero, metrics_serial, metrics_both], ignore_index=True)\n",
    "\n",
    "print(metrics_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3224d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Naive   Subsample     FiveMin         ZMA      TrueQV       K_sub  \\\n",
      "count 1.00000e+03 1.00000e+03 1.00000e+03 1.00000e+03 1.00000e+03 1.00000e+03   \n",
      "mean  1.23433e-02 6.42948e-04 6.82377e-04 6.39856e-04 6.40067e-04 5.79267e+02   \n",
      "std   7.76731e-04 7.58545e-04 7.74844e-04 7.66545e-04 7.65662e-04 3.41897e+01   \n",
      "min   1.14913e-02 9.67219e-05 1.16751e-04 1.07587e-04 1.12759e-04 4.89000e+02   \n",
      "25%   1.18669e-02 1.85240e-04 2.07766e-04 1.66619e-04 1.66955e-04 5.55000e+02   \n",
      "50%   1.20447e-02 2.99407e-04 3.36323e-04 2.91044e-04 2.86134e-04 5.78000e+02   \n",
      "75%   1.25329e-02 7.70859e-04 8.13340e-04 7.86870e-04 7.85777e-04 6.01000e+02   \n",
      "max   1.70444e-02 5.54294e-03 5.50729e-03 5.66832e-03 5.64889e-03 7.16000e+02   \n",
      "\n",
      "            K_zma  \n",
      "count 1.00000e+03  \n",
      "mean  2.55190e+01  \n",
      "std   1.54078e+00  \n",
      "min   2.20000e+01  \n",
      "25%   2.40000e+01  \n",
      "50%   2.50000e+01  \n",
      "75%   2.60000e+01  \n",
      "max   3.20000e+01  \n"
     ]
    }
   ],
   "source": [
    "def simulate_heston_bates(\n",
    "    mu=0.05,\n",
    "    kappa=5.0,\n",
    "    theta=0.04,\n",
    "    xi=0.5,\n",
    "    rho=-0.5,\n",
    "    v0=0.04,\n",
    "    T=1/252,\n",
    "    seconds_per_day=int(6.5 * 3600),\n",
    "    jump_intensity=1260,\n",
    "    jump_mu=0.0,\n",
    "    jump_sigma=0.02\n",
    "):\n",
    "    \"\"\"\n",
    "    Simulate one trading day under the Heston–Bates model (no intraday seasonality).\n",
    "    - X: log-price\n",
    "    - v: variance\n",
    "    - J: jump multipliers (1 if no jump, lognormal if jump)\n",
    "    - dt: time step\n",
    "    \"\"\"\n",
    "    dt = T / seconds_per_day\n",
    "    X = np.empty(seconds_per_day + 1)\n",
    "    v = np.empty(seconds_per_day + 1)\n",
    "    J = np.ones(seconds_per_day + 1)\n",
    "\n",
    "    X[0] = np.log(100.0)\n",
    "    v[0] = v0\n",
    "\n",
    "    L = np.linalg.cholesky([[1, rho], [rho, 1]])\n",
    "\n",
    "    for i in range(1, seconds_per_day + 1):\n",
    "        z = np.random.randn(2)\n",
    "        dW1, dW2 = (L @ z) * np.sqrt(dt)\n",
    "\n",
    "        dv = kappa * (theta - v[i-1]) * dt + xi * np.sqrt(max(v[i-1], 0)) * dW2\n",
    "        v[i] = max(v[i-1] + dv, 0.0)\n",
    "\n",
    "        if np.random.rand() < jump_intensity * dt:\n",
    "            J[i] = np.random.lognormal(mean=jump_mu, sigma=jump_sigma)\n",
    "        else:\n",
    "            J[i] = 1.0\n",
    "\n",
    "\n",
    "        X[i] = (\n",
    "            X[i-1]\n",
    "            + (mu - 0.5 * v[i-1]) * dt\n",
    "            + np.sqrt(v[i-1]) * dW1\n",
    "            + np.log(J[i])\n",
    "        )\n",
    "\n",
    "    return X, v, J, dt\n",
    "\n",
    "def monte_carlo_bates(\n",
    "    M=500,\n",
    "    noise_sd=0.0005,\n",
    "    mu=0.05,\n",
    "    kappa=5.0,\n",
    "    theta=0.04,\n",
    "    xi=0.5,\n",
    "    rho=-0.5,\n",
    "    v0=0.04,\n",
    "    T=1/252,\n",
    "    seconds_per_day=int(6.5 * 3600),\n",
    "    jump_intensity=300,\n",
    "    jump_mu=0.0,\n",
    "    jump_sigma=0.02\n",
    "):\n",
    "    \"\"\"\n",
    "    Monte Carlo over M days for Heston–Bates without U-shape seasonality.\n",
    "    For each day:\n",
    "      1) Simulate Heston–Bates log-prices X, variances v, jumps J.\n",
    "      2) Add microstructure noise to X → Y.\n",
    "      3) Compute true quadratic variation: ∫_0^T v_t dt + sum_jumps (log J)^2.\n",
    "      4) Compute integrated quarticity: ∫_0^T v_t^2 dt.\n",
    "      5) Determine optimal subsampling K_sub and ZMA K_zma.\n",
    "      6) Compute:\n",
    "         - Naive RV (1-second)\n",
    "         - Subsampled RV\n",
    "         - 5-minute RV\n",
    "         - ZMA RV\n",
    "    Returns a DataFrame with columns: Naive, Subsample, FiveMin, ZMA, TrueQV.\n",
    "    \"\"\"\n",
    "    var_eps = noise_sd**2\n",
    "    out = {\n",
    "        'Naive':     np.empty(M),\n",
    "        'Subsample': np.empty(M),\n",
    "        'FiveMin':   np.empty(M),\n",
    "        'ZMA':       np.empty(M),\n",
    "        'TrueQV':    np.empty(M),\n",
    "        'K_sub':     np.empty(M),\n",
    "        'K_zma':     np.empty(M)\n",
    "    }\n",
    "\n",
    "    for i in range(M):\n",
    "        X, v, J, dt = simulate_heston_bates(\n",
    "            mu=mu, kappa=kappa, theta=theta, xi=xi,\n",
    "            rho=rho, v0=v0, T=T, seconds_per_day=seconds_per_day,\n",
    "            jump_intensity=jump_intensity, jump_mu=jump_mu, jump_sigma=jump_sigma\n",
    "        )\n",
    "\n",
    "        Y = add_micro_noise(X, noise_sd)\n",
    "\n",
    "        cont_var = np.sum(v[:-1]) * dt\n",
    "\n",
    "        jump_var = np.sum(np.log(J)**2)\n",
    "\n",
    "        true_qv = cont_var + jump_var\n",
    "        out['TrueQV'][i] = true_qv\n",
    "\n",
    "        iq = np.sum(v[:-1]**2) * dt\n",
    "\n",
    "        bar_n_star = ((T / (6 * var_eps**2)) * iq)**(1/3)\n",
    "        K_sub = max(1, int(round(seconds_per_day / bar_n_star)))\n",
    "        out['K_sub'][i] = K_sub\n",
    "\n",
    "        eta2 = (4/3) * iq\n",
    "        c_star = ((16 * var_eps**2) / (T * eta2))**(1/3)\n",
    "        K_zma = max(1, int(round(c_star * (seconds_per_day**(2/3)))))\n",
    "        out['K_zma'][i] = K_zma\n",
    "\n",
    "        out['Naive'][i] = naively_sampled_rv(Y)\n",
    "\n",
    "        subs_rv, _ = subsampled_rv(Y, K_sub)\n",
    "        out['Subsample'][i] = subs_rv\n",
    "\n",
    "        idx_5min = np.arange(0, seconds_per_day + 1, 300)\n",
    "        Y_5min = Y[idx_5min]\n",
    "        out['FiveMin'][i] = np.sum(np.diff(Y_5min)**2)\n",
    "\n",
    "        out['ZMA'][i] = zma_rv(Y, K_zma)\n",
    "\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "df = monte_carlo_bates(\n",
    "    M=1000,\n",
    "    noise_sd=0.0005,\n",
    "    mu=0.05, kappa=5.0, theta=0.04, xi=0.5, rho=-0.5, v0=0.04,\n",
    "    jump_intensity=300, jump_mu=0.0, jump_sigma=0.02\n",
    ")\n",
    "\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e239f1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20250202</td>\n",
       "      <td>23:00:00</td>\n",
       "      <td>5949.989</td>\n",
       "      <td>5949.989</td>\n",
       "      <td>5945.498</td>\n",
       "      <td>5945.498</td>\n",
       "      <td>0.00469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20250202</td>\n",
       "      <td>23:00:01</td>\n",
       "      <td>5942.751</td>\n",
       "      <td>5944.492</td>\n",
       "      <td>5942.504</td>\n",
       "      <td>5943.754</td>\n",
       "      <td>0.00737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20250202</td>\n",
       "      <td>23:00:02</td>\n",
       "      <td>5942.242</td>\n",
       "      <td>5942.501</td>\n",
       "      <td>5937.501</td>\n",
       "      <td>5937.501</td>\n",
       "      <td>0.00938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20250202</td>\n",
       "      <td>23:00:03</td>\n",
       "      <td>5936.236</td>\n",
       "      <td>5936.236</td>\n",
       "      <td>5934.001</td>\n",
       "      <td>5934.245</td>\n",
       "      <td>0.00670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20250202</td>\n",
       "      <td>23:00:04</td>\n",
       "      <td>5934.179</td>\n",
       "      <td>5934.179</td>\n",
       "      <td>5927.989</td>\n",
       "      <td>5927.989</td>\n",
       "      <td>0.00804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3384376</th>\n",
       "      <td>20250525</td>\n",
       "      <td>23:59:51</td>\n",
       "      <td>5851.634</td>\n",
       "      <td>5851.634</td>\n",
       "      <td>5851.498</td>\n",
       "      <td>5851.498</td>\n",
       "      <td>0.00122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3384377</th>\n",
       "      <td>20250525</td>\n",
       "      <td>23:59:55</td>\n",
       "      <td>5851.245</td>\n",
       "      <td>5851.245</td>\n",
       "      <td>5851.245</td>\n",
       "      <td>5851.245</td>\n",
       "      <td>0.00061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3384378</th>\n",
       "      <td>20250525</td>\n",
       "      <td>23:59:57</td>\n",
       "      <td>5851.742</td>\n",
       "      <td>5851.742</td>\n",
       "      <td>5851.742</td>\n",
       "      <td>5851.742</td>\n",
       "      <td>0.00061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3384379</th>\n",
       "      <td>20250525</td>\n",
       "      <td>23:59:58</td>\n",
       "      <td>5851.625</td>\n",
       "      <td>5851.754</td>\n",
       "      <td>5851.625</td>\n",
       "      <td>5851.754</td>\n",
       "      <td>0.00122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3384380</th>\n",
       "      <td>20250525</td>\n",
       "      <td>23:59:59</td>\n",
       "      <td>5851.878</td>\n",
       "      <td>5851.878</td>\n",
       "      <td>5851.733</td>\n",
       "      <td>5851.733</td>\n",
       "      <td>0.00122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3384381 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date Timestamp      Open      High       Low     Close   Volume\n",
       "0        20250202  23:00:00  5949.989  5949.989  5945.498  5945.498  0.00469\n",
       "1        20250202  23:00:01  5942.751  5944.492  5942.504  5943.754  0.00737\n",
       "2        20250202  23:00:02  5942.242  5942.501  5937.501  5937.501  0.00938\n",
       "3        20250202  23:00:03  5936.236  5936.236  5934.001  5934.245  0.00670\n",
       "4        20250202  23:00:04  5934.179  5934.179  5927.989  5927.989  0.00804\n",
       "...           ...       ...       ...       ...       ...       ...      ...\n",
       "3384376  20250525  23:59:51  5851.634  5851.634  5851.498  5851.498  0.00122\n",
       "3384377  20250525  23:59:55  5851.245  5851.245  5851.245  5851.245  0.00061\n",
       "3384378  20250525  23:59:57  5851.742  5851.742  5851.742  5851.742  0.00061\n",
       "3384379  20250525  23:59:58  5851.625  5851.754  5851.625  5851.754  0.00122\n",
       "3384380  20250525  23:59:59  5851.878  5851.878  5851.733  5851.733  0.00122\n",
       "\n",
       "[3384381 rows x 7 columns]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500 = pd.read_csv('USA500IDXUSD.csv')\n",
    "sp500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e32fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df_orig):\n",
    "    df = df_orig.copy()\n",
    "    df['Datetime'] = pd.to_datetime(df['Date'].astype(str) + ' ' + df['Timestamp'])\n",
    "    df.set_index('Datetime', inplace=True)\n",
    "    df['log_close'] = np.log(df['Close'])\n",
    "    return df\n",
    "\n",
    "def realized_quadpower_quarticity(log_prices):\n",
    "    returns = np.diff(log_prices)\n",
    "    M = len(returns)\n",
    "    prod4 = (\n",
    "        pd.Series(np.abs(returns))\n",
    "          .rolling(window=4)\n",
    "          .apply(np.product, raw=True)\n",
    "          .dropna()\n",
    "          .values\n",
    "    )\n",
    "    return (M * (np.pi**2) / 4) * prod4.sum()\n",
    "\n",
    "def daily_rv_estimators(df):\n",
    "    df = df.copy()\n",
    "    T = 1/252\n",
    "    results = []\n",
    "    for date, group in df.groupby(df.index.date):\n",
    "        lp = group['log_close'].values\n",
    "        n = len(lp) - 1\n",
    "        if n < 1:\n",
    "            continue\n",
    "\n",
    "        full_rv = naively_sampled_rv(lp)\n",
    "        var_eps_hat = full_rv / (2 * n)\n",
    "\n",
    "        RQQ = realized_quadpower_quarticity(lp)\n",
    "\n",
    "        if var_eps_hat > 0 and RQQ > 0:\n",
    "            bar_n_star = ((T / (6 * var_eps_hat**2)) * RQQ)**(1/3)\n",
    "            K_boost = max(1, int(round(n / bar_n_star)))\n",
    "        else:\n",
    "            K_boost = 1\n",
    "\n",
    "        if var_eps_hat > 0 and RQQ > 0:\n",
    "            eta2 = (4/3) * RQQ\n",
    "            c_star = ((16 * var_eps_hat**2) / (T * eta2))**(1/3)\n",
    "            K_zma = max(1, int(round(c_star * n**(2/3))))\n",
    "        else:\n",
    "            K_zma = 1\n",
    "\n",
    "        rv_naive = full_rv\n",
    "\n",
    "        rv_boost = subsampled_rv(lp, K_boost)\n",
    "\n",
    "        rv_zma = zma_rv(lp, K_zma)\n",
    "\n",
    "        idx_5min = np.arange(0, len(lp), 300)\n",
    "        lp_5min = lp[idx_5min]\n",
    "        rv_5min = naively_sampled_rv(lp_5min)\n",
    "\n",
    "        results.append({\n",
    "            'date':      date,\n",
    "            'var_eps':   var_eps_hat,\n",
    "            'RQQ':       RQQ,\n",
    "            'K_boost':   K_boost,\n",
    "            'K_zma':     K_zma,\n",
    "            'RV_naive':  rv_naive,\n",
    "            'RV_5min':   rv_5min,\n",
    "            'RV_boost':  rv_boost,\n",
    "            'RV_ZMA':    rv_zma\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results).set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "045b68a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loaded_1year = pd.read_csv('1 year.csv')\n",
    "df_prepared_1year = prepare_data(df_loaded_1year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a4de854c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 var_eps           RQQ  K_boost  K_zma      RV_naive  \\\n",
      "date                                                                   \n",
      "2024-05-26  7.837688e-10  1.502585e-12       48      7  8.872263e-07   \n",
      "2024-05-27  6.560155e-10  1.233080e-10      109      7  8.230370e-06   \n",
      "2024-05-28  1.076464e-09  1.925602e-09      165      8  3.675480e-05   \n",
      "2024-05-29  1.038862e-09  3.625128e-09      166      7  4.494947e-05   \n",
      "2024-05-30  1.198683e-09  4.049843e-09      168      8  4.961109e-05   \n",
      "...                  ...           ...      ...    ...           ...   \n",
      "2025-05-20  9.220182e-10  5.330443e-09      209      8  6.203338e-05   \n",
      "2025-05-21  1.544324e-09  3.177101e-08      194      7  1.237189e-04   \n",
      "2025-05-22  1.383416e-09  1.577374e-08      227      8  1.107120e-04   \n",
      "2025-05-23  2.492150e-09  8.562416e-08      198      7  2.060908e-04   \n",
      "2025-05-25  2.836448e-09  1.407859e-09       83      7  2.298658e-05   \n",
      "\n",
      "                 RV_5min      RV_boost        RV_ZMA  \n",
      "date                                                  \n",
      "2024-05-26  1.530939e-07  5.075697e-07  3.990060e-07  \n",
      "2024-05-27  6.295796e-06  5.301860e-06  4.747856e-06  \n",
      "2024-05-28  2.790462e-05  2.831434e-05  2.912997e-05  \n",
      "2024-05-29  3.327074e-05  3.347495e-05  3.416333e-05  \n",
      "2024-05-30  3.226025e-05  3.121348e-05  3.995437e-05  \n",
      "...                  ...           ...           ...  \n",
      "2025-05-20  5.111071e-05  5.319816e-05  5.560661e-05  \n",
      "2025-05-21  1.191362e-04  1.205058e-04  1.125022e-04  \n",
      "2025-05-22  9.475467e-05  9.627411e-05  9.869641e-05  \n",
      "2025-05-23  2.617203e-04  2.176080e-04  2.051403e-04  \n",
      "2025-05-25  3.193534e-05  2.078970e-05  2.016800e-05  \n",
      "\n",
      "[312 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "daily_metrics_1year = daily_rv_estimators(df_prepared_1year)\n",
    "print(daily_metrics_1year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd8646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_log_1year = df_prepared_1year['log_close'].resample('D').last().rename('log_close')\n",
    "daily_log_1year.index = daily_log_1year.index.date\n",
    "combined_1year = daily_metrics_1year.merge(\n",
    "    daily_log_1year,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f6f78614",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_1year.to_csv('daily_rv_estimators_1year_with_log3.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
